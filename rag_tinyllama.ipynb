{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad6d5d4-624c-452d-9112-05909e1c11be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\" \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My RAG Project\"\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68a3d1e0-7e51-4d23-9c68-39c0d5a41e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from the web page.\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://www.promptingguide.ai/research/rag\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s) from the web page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9ad6f03-3c52-442e-a8fe-8867c69f63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document split into 63 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Original document split into {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71ce37cb-d7f7-4402-8d8c-247ef5e85af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Embeddings model initialized.\n",
      "Vector store saved successfully to faiss_index folder.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"HuggingFace Embeddings model initialized.\")\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(f\"Vector store saved successfully to faiss_index folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7f8267e-0cf5-477e-860f-138a9d976e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created from the vector store.\n",
      "LLM initialized: Ollama with model tinyllama\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Retriever created from the vector store.\")\n",
    "llm =Ollama(model= \"tinyllama\")\n",
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "model_kwargs={\"temperature\": 0.5, \"max_length\": 512} \n",
    "print(f\"LLM initialized: Ollama with model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa56ed14-140f-4eb0-8983-de05434f2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG prompt pulled from LangChain Hub.\n",
      "Combine documents chain created.\n",
      "Full RAG chain created.\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"RAG prompt pulled from LangChain Hub.\")\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "print(\"Combine documents chain created.\")\n",
    "rag_chain = ({\n",
    "    \"context\": itemgetter(\"input\") | retriever,\n",
    "     \"question\": itemgetter(\"input\"),\n",
    "    }\n",
    "              | combine_docs_chain\n",
    ")\n",
    "print(\"Full RAG chain created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ad33c4c-2c06-4acf-abae-1e76d8865cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RIGHT WAY COMPUTERS\\AppData\\Local\\Temp\\ipykernel_1348\\3645525293.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response= qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The research papers below highlight key insight and latest development trends in RAG systems, which include the development of advanced paradigms for customization and further performance and utility across a wide range of domains. There is a huge demand for RAG applications that have accelerated the development of methods to improve different components of RAG systems, including hybrid methodologies, self-retrieval techniques, evaluation tools, and metrics. The figure below provides a recap of the RAG ecosystem, techniques to enhance RAG, challenges, and other related aspects covered in these overviews:\n",
      "\n",
      "As introduced here (opens in a new tab), RAG can be defined as:\n",
      "\n",
      "RAG takes input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt, and RAG produces the final output. This makes RAG adaptive for situations where facts could evolve over time. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.\n",
      "\n",
      "In short, RAG can help reduce issues of hallucination or performance when addressing problems in highly evolving environments. While RAG has primarily involved the optimization of pre-training methods like ChaTGP and MixTraL, current approaches have largely shifted to combining the strengths of RAG and powerful fine-tuned models such as ChATGP and MixTraL.\n",
      "\n",
      "As mentioned earlier, some research papers cover the following topics:\n",
      "\n",
      "1. Hybrid Methodologies: This paper discusses the use of different types of methods to enhance RAG.\n",
      "\n",
      "2. Self-Retrieval Techniques: This paper describes self-retrieval techniques that utilize pre-trained language models for retrieving relevant/supporting documents.\n",
      "\n",
      "3. Evaluation Tools and Metrics: This paper introduces a new evaluation framework, called ATLAS, that allows for the creation of user-friendly and interpretable evaluation results.\n",
      "\n",
      "4. Challenges in RAG Systems: This paper explores challenges associated with RAG systems, including the difficulty of adapting to evolving environments, the need for fine-tuning, and the need for large-scale pre-training data sets.\n",
      "\n",
      "In conclusion, RAG is a rapidly evolving field that has significantly impacted various aspects of language modeling, such as language model design, pre-training methods, and evaluation frameworks. With more advancements in this area, we can expect even better applications of RAG to solve complex and challenging problems in the future.\n"
     ]
    }
   ],
   "source": [
    "query =\"define RAG?\"\n",
    "response= qa_chain.run(query)\n",
    "print(\"ANSWER:\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63bdf9-b2f7-4c33-83c9-d9dae000f027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_new_env)",
   "language": "python",
   "name": "rag_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
